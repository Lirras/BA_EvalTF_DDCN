Künstliche Intelligenz (KI) ist mittlerweile selbst außerhalb der Informatik einer breiten Öffentlichkeit bekannt. Die zugrunde liegende 
Technologie basiert in der Regel auf künstlichen neuronalen Netzen, die üblicherweise in einem einzigen Schritt vollständig konstruiert und 
trainiert werden. Dieser Prozess ist jedoch zeit- und rechenintensiv, weshalb alternative Verfahren wie die Cascade-Correlation-Methode 
entwickelt wurden \cite{cascor}. Erste Untersuchungen zeigten, dass diese Kaskadierungsstrategie bereits bei kleineren Netzarchitekturen 
zufriedenstellende Ergebnisse liefert. Darauf aufbauend wurden verschiedene erweiterte Netzwerkstrukturen und Kaskadierungsverfahren 
entwickelt \cite{cascade_network_architectures}, \cite{Constructive_Cascade}, \cite{deep_cascade_learning}.

Ein wesentlicher Vorteil kaskadierender Netzwerke liegt in ihrer modularen Struktur, die eine flexible Anpassung an unterschiedliche 
Datensätze und Aufgabenstellungen ermöglicht \cite{phd_deep_cascade}, \cite{transfer_learning}, \cite{survey_transfer}. Die vorliegende 
Arbeit ist im Kontext dieser Transferfähigkeit einzuordnen. Ziel ist es, die Leistungsfähigkeit verschiedener Netzwerkarchitekturen zu 
evaluieren und spezifische Herausforderungen im Zusammenhang mit Transferlernen (TF) und Kaskadierung zu analysieren.
