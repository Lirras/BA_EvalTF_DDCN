Hier werden alle Netze bearbeitet und überprüft wieviel Zeit sie für ihr Training benötigten. 
Diese wird in jedem Plot angezeigt. Sie ist generell stark abhängig davon, wieviele Datensamples genutzt werden und 
wieviele maximale Epochen erlaubt sind. Deshalb werden hier jeweils gleich viele Datensamples für die Klassifikationsnetze und 
Regressionsnetze verwendet, sowie eine gleiche Gesamtepochenanzahl. 

Dazu wird jedes Mal der kleinste Targetdatensatz und der größte Sourcedatensatz genutzt, bis auf das 2DConv-Netzwerk, wenn TF verwendet wird. 
Solange nicht TF verwendet wird, wird nur der kleinste Targetdatensatz genommen. Jedes Klassifikationsnetz 
wird auf eine Gesamtanzahl von 40 Epochen trainiert. Wenn TF gemacht wird, dann nach 20 Epochen. Bei Regressionsnetzen wird auf 80 Epochen 
trainiert. TF wird nach 30 Epochen gemacht. Tatsächliche Graphen werden hier nicht verwendet, da es zuviele sind und die meisten an 
anderen Stellen in ähnlicher Form bereits vorkommen. Um diese Graphen zu kontrollieren, schaut bitte auf das GitHub Repo unter 
\url{https://github.com/Lirras/BA_EvalTF_DDCN/tree/main/Plots/ba_plots/timing}. 
Es wird hier keine Early-Stopping Metrik genutzt. 

Dabei werden alle Klassifikationsnetze mit denselben Inputdaten gespeist, ebenso wie bei allen Regressionsnetze, was zur besseren 
Vergleichbarkeit jeweils untereinander führt. 
Die jeweiligen Netzversionen zwischen Cascade TF, Cascade und Complete haben dieselben Layer, sowie diegleiche Anzahl. 

Hier nun die Tabelle mit allen Zeiten der Netze in einer vergleichbaren Variante: \newline

\begin{table}[h!]
    \begin{center}
        \caption{Timingvergleich in Sekunden}
        \label{tab:time}
        \begin{tabular}{l|l|l|l}
            \textbf{Netzwerk} & \textbf{Cascade TF} & \textbf{Cascade} & \textbf{Complete} \\
            \hline
            ConvMaxPool & 78 & 25 & 20 \\
            1DConv & 207 & 34 & 30 \\
            2DConv & 23 & 24 & 40 \\
            ClassOneDense & 79 & 28 & 13 \\
            RegressionTwo & 11 & 12 & 17 \\
            OneLayer & 16 & 18 & 11
        \end{tabular}
    \end{center}
\end{table}

In Table 3.1 ist die Spalte Cascade TF, die Kaskadennetzwerke mit TF beeinhaltet. Die Spalte Cascade ist diejenige, in der die Netze nur auf dem 
Targetdatensatz gelernt haben, aber Kaskadennetzwerke sind und die Spalte Complete sind die Lernzeiten der Netze, die weder TF machen noch 
Kaskadennetzwerke sind, sondern dessen Layer vor dem Training bereits feststanden und komplett in einem auf dem Targetdatensatz gelernt wurden. 

Auffällig ist, dass die Regressionsnetze keine große Zeitveränderung haben. Mitunter benötigt mit TF weniger Zeit als ohne, was daran liegt, dass 
der Targetdatensatz der Regression ein wenig größer ist als der Sourcedatensatz. Beide sind allerdings mit etwas über 200 Datensamples sehr klein. 

Ebenso brauchen alle Klassifikationsnetze länger mit TF als ohne, was daran liegt, dass sie zuerst auf dem Sourcedatensatz trainieren, welcher mit 
48000 Trainingsdaten recht groß ausfällt, während die Anderen direkt auf dem kleinen Targetdatensatz, der mit 732 Datensamples klein ist, 
trainieren. 

Da Cascade TF mit dem Sourcedatensatz ist und die anderen Netze nur auf dem Targetdatensatz arbeiten, sind diese meist kürzer. Allerdings 
benötigen die Complete Netzwerke, die ohne Kaskadierung sind, noch weniger Zeit. Dies wird an der Implementierung der Netze liegen, da bei jedem 
Kaskadennetzwerk das Outputlayer in jeder Kaskade mit berechnet wird, während bei den Complete Netzwerken nur ein einziges Outputlayer existiert. 
Des Weiteren werden keine extra Predictions und keine Berechnung der Augmented Vectors gemacht. 
