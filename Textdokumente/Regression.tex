Bei einer Regression kommt es dazu, dass das NN sich mithilfe 
der Daten der korrekten Funktion annähert.

\section{Datensätze}
    Die genutzten Datensätze sollen zunächst Boston Housing und California Housing sein, da 
    beide dasselbe Ziel mit der Annäherung an den Preis des Hauses haben. 
    Boston Housing hat ein ethnisches Problem, da es eine Spalte gibt, die den Preis in 
    Abhängigkeit zur Anzahl der in der Nähe lebenden Schwarzen setzt. Diese wird hier entfernt. 
    Ebenso entfernt werden die Spalten, die kein vernünftiges Gegenüber im California Datensatz 
    haben. Dadurch reduziert sich die Spaltenanzahl auf 3. Bei Boston bleiben RM, AGE und LSTAT 
    über, die im Vergleich halbwegs zu den Spalten AveRoom, HouseAge und MedInc vom California 
    Datensatz passen. 
    RM ist dabei die durchschnittliche Anzahl der Räume pro Wohnung, während AveRoom die 
    durchschnittliche Anzahl pro Haushalt ist. 
    AGE ist dabei der Anteil der Wohnungen, die vor 1940 erbaut worden sind. HouseAge ist 
    hingegen das Durchschnittsalter der Häuser eines Wohnblocks. Diese Spalten müssen 
    aneinander angepasst werden.
    LSTAT ist der prozentuale Anteil der niedrigeren Bevölkerung und MedInc ist das mittlere 
    Einkommen des Wohnblocks. Hier müssen ebenfalls leichte Anpassungen durchgeführt werden. 
    Alle anderen Spalten beider Datensätze sind nicht ähnlich genug, dass zwischen ihnen 
    gelernt werden sollte. Die Erwartung ist, dass alle TF-Netze schlechter abschneiden als die 
    normalen schon allein deshalb, weil durch die Spaltenreduktion sehr viele Daten nicht 
    betrachtet werden. Des Weiteren ist genau das bei der Klassifikation ebenfalls passiert.

\subsection{2LinLayer}
    Ein sehr simples Netz, bestehend aus zwei Hidden Dense (Linear) Layern. 
    Die Metrik, die hier betrachtet wird ist der Mean Absolute Error (MAE). 
    \begin{figure}[htpb]
        \includegraphics[height=5cm]{../Plots/regression/2linlayer/AP_mae.png}
        \includegraphics[height=5cm]{../Plots/regression/2linlayer/mae.png}
        \caption{\label{fig:figure13} MAE Vergleich mit und ohne Preprocessing}
    \end{figure}
    Der erste Graph hat einen leicht höheren MAE, was am Preprocessing der Daten liegt. 
    Der einzige Unterschied ist der, dass beim zweiten die Spalte LSTAT nicht verändert wird. 
    Diese ist im Boston Housing grob Antiproportional zu dem tatsächlichen Preis. Da diese als 
    Vor-Training für die Spalte Median Income von dem California Housing, die grob proportional 
    zum Preis ist, gedacht ist, wurde aus der LSTAT-Spalte eine proportionale Spalte gemacht, 
    damit es besser zu Median Income passt. Es wird dabei allerdings schlechter. 
    Was aber daran liegt, dass die Learningrate bei diesem Test immer größer wird. Dies hat zur 
    Folge, dass das spätere Layer dies noch umbauen kann, denn bei einem späteren Test mit 
    einer immer kleiner werdenden Learningrate verbessert dieses Preprocessing das 
    Ergebnis etwas. 
    Das bereits bekannte Verhalten nach TF trifft auch hier wieder zu: Nach dem TF wird es 
    erst sehr viel schlechter und fängt sich dann wieder. 
    Der MAE wird auch deshalb sehr viel höher, weil der Zahlbereich des Preises des Hauses im 
    zweiten Datensatz sehr viel größer ist.
    Im Weiteren werden beide Layer jeweils mit hundert Epochen trainiert und die learningrate, 
    sowie die Antiproportionalität der LSTAT-Spalte verändert. 
    \begin{figure}[htpb]
        \includegraphics[height=5cm]{../Plots/regression/2linlayer/lr100_AP.png}
        \includegraphics[height=5cm]{../Plots/regression/2linlayer/lr100NAP.png}
        \includegraphics[height=5cm]{../Plots/regression/2linlayer/lr1000_AP.png}
        \includegraphics[height=5cm]{../Plots/regression/2linlayer/lr1000_NAP.png}
        \caption{\label{fig:figure14} MAE: Learningrate Tests mit und ohne Preprocessing}
    \end{figure}
    Wenn die Spalten besser zueinander passen, also beide proportional sind, dann gibt es nach TF 
    einen nicht ganz so starken Anstieg des Errors, aber das Endergebnis ist etwas schlechter. 
    Mit der aktuellen Learningrate-Funktion, die linear abnimmt, wird das Netz nach vielen Epochen 
    sehr stabil, aber auch nicht mehr besser. 
    Dies liegt daran, dass das Netz in ein lokales Minimum läuft und durch die immer kleiner 
    werdende Learningrate dort nicht mehr heraus kommt. Am deutlichsten ist es hier zu sehen: 
    \begin{figure}[htpb]
        \includegraphics[height=5cm]{../Plots/regression/2linlayer/localMinimum.png}
        \caption{\label{fig:figure15} MAE: localMinimum}
    \end{figure}
    Dies ist ein Netz mit Vier Layern. Man sieht jedes mal, wenn ein neues Layer neu angefangen wird 
    am Ausschlag des Graphen, doch es ändert sich nichts.
