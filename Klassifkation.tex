Eine Klassifikation ist dann, wenn ein NN eingegebene Daten zu 
    eindeutigen Klassen zuordnen soll.

\section{Beschreibung der Tätigkeit}
    
    Ausführung auf GPU klappt zurzeit nicht. Warum? Wahrscheinlich, 
    weil das device in jedem Layer händisch bei deren Erstellung mitgegeben 
    werden muss.
    Wieso ist nach dem ersten Linearlayer die Accuracy über 8000\%?
    Es fehlt das Reshape, damit es einen vernünftigen Vergleich 
    zwischen dem Output und den Labeln geben kann.
    
    Wie kann ich es machen, dass es schneller geht bei mehreren Epochen 
    und einem großen Netz? -> Auf GPU geht ja nicht.
    Nicht Layerwise den Output berechnen, sondern bzgl. des gesamten Netzes.

    Abspeichern zwischen den einzelnen Layern, damit die Trainingsphase 
    schneller wird, muss mit Tensoren passieren, die nicht in einer Liste 
    stehen. Dies geht aber nicht, da diese nicht gut ausgelesen werden können. 
    Sprich: In der nächsten Iteration gar nicht, bzw. sie sind dann nicht 
    mehr die benötigten Tensoren.

    Target Layer muss 3-Dimensional sein, wenn die Batch-Size größer als 1 
    ist und auch wenn ein multi-Class-Loss wie Negative Log-Likelihood-Loss 
    genutzt wird. Wie bekommt man das hin?
    Mit einem Reshape der Daten, da es nur mit dem passendem Format klappt. 

    Aber NLLLoss ist schlecht im Verhältnis zu CrossEntropyLoss, weil 
    CrossEntropyLoss in einem Fall NLLLoss und logSoftmax gemeinsam macht.
    Es ist bisher die sinnvollste Loss function.

    Mache lieber eine Batch Size, die auf die Daten aufgeht. Sonst verändert 
    sich der Shape der Tensoren nach dem letzten Durchlauf der Epoche und ist 
    ein anderer Input für das nächste Layer.

    TF mit Domainwechsel ist nur dadurch möglich, dass am Anfang ein 
    Identity-Layer genutzt wird, welches durch ein Conv ersetzt wird, 
    damit die Shapes der Inputs der verschiedenen Datasets zu dem 
    Netzwerk passen.

    Hier wird aber noch einer Möglichkeit gesucht, die Größe der 
    Datensätze so umzubauen, dass es mit der Batch Size glatt aufgeht.

    Freezing wird benötigt, was aber sehr leicht umsetzbar ist, indem 
    der required grad auf False gesetzt wird. Dies sorgt dafür, dass 
    es für die Gradientenberechnung für die neuen Weights die Lernrate 
    auf Null gesetzt ist, sodass es kein Update mehr gibt.

    Direct Cascade funktioniert bereits. Auffällig dabei ist, dass 
    die Performanz des Netzes am Anfang des Trainings des neuen Layers 
    deutlich schlechter wird, dann aber sehr schnell sich dem vorherigen 
    Wert angleicht. Aber es wird mit einem neuen Layer insgesamt nicht 
    signifikant besser, sondern höchstens Minimal mit 2 Prozent maximal. 
    Es kann sogar passieren, dass es dadurch schlechter wird.
    
\section{Inversive Cascade TF Network}

    Der erste Versuch ist nur, um zu schauen, ob es überhaupt funktioniert. 
    Das Ergebnis ist allerdings Negative TF, da die Accuracy schlechter wude. 
    Das Netz wurde mit PyTorch, stochastic Gradient Descent, Optimizer und 
    Backpropagation with Freezing Weights programmiert. 

    Genutzte Datensätze waren MNIST und SVHN. Trainiert wurde wie bei einem 
    Cascade Network üblich Layer für Layer mit jeweils einer Epoche.

    Da es bei den Reshape-Layern ein Problem damit gibt, wenn die Batch Size kein Teiler 
    von der Länge der Datensätze ist, werden in den Epochen alle Daten die im letzten Batch 
    sind, nicht beachtet. Die Größen der Tensoren muss passend sein und genau die wird 
    dadurch verändert, wenn es nicht aufgeht. Dies liegt aber daran, dass Flatten-Layer 
    erst später gemacht werden.

    Die Layer waren wie folgt: 
    \begin{enumerate}
        \item Identity $\rightarrow$ Conv2D 2 (3, 1, 5)
        \item Reshape 1 (-1, 784)
        \item Linearlayer 1 (784, 784)
        \item Reshape 2 (100, 1, 28, 28)
        \item Conv2D 1 (1, 1, 3)
        \item Reshape 3 (-1, 784)
        \item Linearlayer 2 (784, 10)
    \end{enumerate}

    Das Identity-Layer wird deshalb gebraucht, da die Datensätze nicht 
    dieselben Größen haben und dieses durch ein Faltungslayer ausgetauscht 
    werden muss.

    Reshape 1 ist dafür da, die Daten für den Input des Linearlayers vorzubereiten. 
    Die Daten liegen in einem 4D-Tensor vor und müssen aber ein 2D-Tensor sein. 

    Linearlayer 1 ist die erste Berechnung mit sich aktualisierenden Gewichten. 
    Während dem Training hat das erste Layer bereits eine recht hohe Accuracy. 
    Dabei hat dieses Layer genauso viele Input-Kanäle wie Output-Kanäle. 
    Es würde sonst zuviel Information verloren gehen, die die späteren Layer benötigen.

    Reshape 2 bereitet die Daten auf das Convolutionlayer vor, da dieses die Daten 
    in einem 4D-Tensor benötigt.

    Convolutionlayer 1 hat einen input- und einen output-Kanal und einen 3x3-Kern 
    mit einem zero-Padding von 1, damit sich der Shape der Tensoren nicht verändert. 

    Reshape 3 macht aus dem 4D-Tensor wieder ein 2D-Tensor.

    Linearlayer 2 hat nur so viele Output-Kanäle, wie es Klassen gibt, denn es 
    ist das letzte Layer. 

    Dies ist alles noch ohne TF.
    Convolutionlayer 2 fügt das vorne im Netz hinzu, da es benötigt wird, die 
    Daten von SVHN von 32x32 auf 28x28 herunterzubrechen. Dafür wird das Identity-Layer 
    entfernt. Nach einer Trainingsepoche ist die Accuracy sehr schlecht.

    Das Netz mit dem Convolutionlayer 2 und den anderen Daten ist also deutlich schlechter 
    als auf dem Source-Datensatz, aber es ist besser als wenn ein Convolutionlayer ohne Vorarbeit 
    auf die Daten angewendet wird.

    Wenn mehr Epochen genutzt werden, wird die Performanz nach dem TF manchmal 
    noch schlechter. Damit sich hier noch etwas ändert muss das Erste Layer noch 
    einmal mit den neuen Daten allein trainiert werden.

    Die Idee einem fertigen Kaskadennetzwerk das erste Layer zu ersetzen um TF zu machen, also 
    dem Netz für TF vorne Layer hinzuzufügen, führt zu einer Verschlechterung und sollte 
    unterlassen werden.

    Hier ein Beispiel der Werte des Netzes:

    \begin{table}[h!]
        \begin{tabular}{l|l|l|l}
            Layer & Epochen & Accuracy & Loss \\
            \hline
            Linear 1 & 1 & 82.6 & 0.98 \\
            & 2 & 85.0 & 0.68 \\
            & 3 & 86.2 & 0.58 \\
            & 4 & 86.6 & 0.52 \\
            & 5 & 87.2 & 0.49\\
            Conv 1 & 1 & 87.2 & 0.40 \\
            & 2 & 87.1 & 0.40 \\
            & 3 & 87.1 & 0.40 \\
            & 4 & 87.1 & 0.40 \\
            & 5 & 87.2 & 0.40 \\
            Linear 2 & 1 & 86.8 & 0.45 \\
            & 2 & 87.7 & 0.40 \\
            & 3 & 88.0 & 0.39 \\
            & 4 & 87.6 & 0.39 \\
            & 5 & 88.0 & 0.37 \\
            Conv 2 & 1 & 10.8 & 2.75 \\
            & 2 & 9.6 & 2.77 \\
            & 3 & 9.6 & 2.96 \\
            & 4 & 9.4 & 2.94 \\
            & 5 & 9.4 & 3.02 \\
            & 6 & 9.5 & 3.12 \\
        \end{tabular}
    \end{table}

    Die Idee das Conv 2 vorne anzufügen, führt dazu, dass sich das Netz verschlechtert. 
    Bei mehreren Epochen wird es ebenfalls erst einmal schlechter.

    % Wieso bringen einfache Conv Layer nichts?

    Convolutionlayer sind hier aber deshalb so schlecht, weil sie alle nur einen Output-
    Channel haben und sie pro Output-Channel nur ein Feature lernen können.
    Also werden mehrere verwendet werden müssen und diese dann weiter verarbeitet. 
    Da es hier kein Pooling oder Flatten-Layer gibt, werden hier viele Informationen 
    einfach verloren.

    % Warum ist es overall so schlecht?

    Das dürfte daran liegen, dass Convolutionlayer aktuell noch sehr schlecht generell sind. 
    Aber auch daran, dass die Gewichte hinten im Netz genauer sein müssen und diese nicht 
    geupdatet werden. 

\section{Lin4Conv1 Compose Database Network}
    Datensatzwechsel von MNIST zu SVHN mit Reshape und Database-Compose von SVHN. 
    Es ist alles ähnlich zu dem vorherigen, bis auf: 
    Das erste Layer wird nicht ausgetauscht. Am Ende wird ein Linearlayer hinzugefügt. 
    Die Daten des SVHN-Datensatzes liegen in RGB vor und haben normalerweise 3 Channels. 
    Diese werden zusammengefasst und zu einem Channel verändert.

    Die Layer sind wie folgt: 
    \begin{enumerate}
        \item Identity
        \item Linearlayer 1 (784, 784)
        \item Conv2D (1, 1, 3)
        \item Linearlayer 2 (784, 784)
        \item TF
        \item Linearlayer 3 (784, 10)
    \end{enumerate}

    Das Ergebnis ist, dass das Netz vor dem TF eine Accuracy von 87.6\% hatte. Nach 
    dem TF allerdings eine von 21.1\%. Das ist sehr viel schlechter, aber es ist besser als 
    wenn ein einfaches Conv2D-Layer ohne weitere Verarbeitung genutzt wird. Das landet 
    nämlich bei circa 3\%. Da bei diesem Netz ein solches Conv2D-Layer im ersten Teil 
    genutzt wird, liegt es wahrscheinlich an jenem Layer, jedoch ist es mit TF schon 
    deutlich besser als ohne. Allerdings kann dies genauso gut an dem vorhergegangenen 
    Linearlayer liegen, dass es doch so gut ist. Ein weiterer Grund weshalb es recht schlecht 
    sein könnte ist der Fall, dass der letzte Linearlayer nur noch 10 Input-Features hat.

    Wenn das Conv2D-Layer rausgenommen wird, passiert folgendes: 
    Vor dem TF eine Accuracy von 87.9\% und danach eine von: 19.5\%. Also hat das Entfernen des 
    Conv2D-Layers nichts verändert. Dies liegt daran, dass es zu wenige Features als Input 
    für das letzte Linearlayer gibt und dieses deshalb pro Epoche immer schlechter wird. 
    Dieses Verhalten gibt es auch ohne TF. 

    Die Features für die Linearlayer mehr machen ergibt: 
    Vorher: 87.9\%, Nachher: 10.7\%. Das Netz hat vermutlich Overfitting, weil das 
    letzte Layer zu mächtig ist und zu viele Veränderungen durchführen kann. 

    Vergleichswert: SVHN ohne TF mit einem Linearlayer hat Accuracy von: 22.7\%.
    Daraus folgt, dass das erste Netz tatsächlich bereits ein recht gutes TF vollbracht hat. 
    Wieso ist aber dieser Datensatz so schlecht mit einem Linearlayer zu lösen?

    Die Idee, dass es eventuell am Compose liegt stimmt nicht wirklich, aber es wird besser, 
    wenn Source- und Target-Datensatz beide ein Compose haben.
    Nachdem die Testfunktion repariert wurde, sind die Ergebnisse minimal besser. 

    Das Resultat ist, dass dieses Netzwerk eine Accuracy von 30.2\% hat, was sehr viel 
    besser ist als ohne TF.

\section{2LayerLinear}
    Ein Zwei-Layer Netz mit nur Linearlayern. Das erste trainiert mit MNIST, das zweite 
    mit SVHN bei jeweils fünf Epochen. Ergebnis: Nach MNIST ACC: 90.4\%; Nach SVHN: ACC: 22.4\%. 
    Es gibt hier nicht wirklich einen Unterschied, ob MNIST ein upscaling oder SVHN ein downscaling 
    bekommt.

\section{3Conv3Linear}
    Convolutionlayer laufen jetzt und werden sehr schnell sehr viel besser je mehr hintereinander 
    gesetzt werden, aber sobald auf Linearlayer gewechselt wird, wird es wieder etwas schlechter.

    Aber dieses Netz hat das erste Mal tatsächlich positive TF. Das Netz mit dem ersten 
    positiven TF hat die Layer und eine ACC bei einer Epoche: 
    \begin{enumerate}
        \item Conv2D (1, 4, 7); ACC: 5.8\%
        \item Conv2D (4, 16, 5); ACC: 26.2\%
        \item Conv2D (16, 64, 3); ACC: 32.4\%
        \item Linear (64, 64); ACC: 22.5\%
        \item Linear (64, 64); ACC: 9.3\%
        \item Linear (64, 64); ACC: 10.6\%
        \item TF
        \item Linear (64, 64); ACC: 19.6\%
    \end{enumerate}

    Mit fünf Epochen ist die Accuracy vor TF bei 41.3\% und danach bei 20.2\%.

\section{Conv2DMidTF}
    Vier Convolutionlayer und in der Mitte von ihnen TF.
    Aufbau mit ACC nach 5 Epochen: 
    \begin{enumerate}
        \item Linear (1024, 1024); ACC: 89.8\%
        \item Conv2D (1, 8, 5); ACC: 27.4\%
        \item Conv2D (8, 32, 5); ACC: 39.7\%
        \item TF
        \item Conv2D (32, 64, 3); ACC: 18.0\%
        \item Conv2D (64, 100, 3); ACC: 19.8\%
        \item Linear (100, 100); ACC: 19.6\%
        \item Linear (100, 100); ACC: 19.6\%
        \item Linear (100, 100); ACC: 19.5\%
    \end{enumerate}
    Auch hier ist es wieder so, dass das SVHN-Netz nicht besser wird mit TF.

